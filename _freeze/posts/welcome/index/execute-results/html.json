{
  "hash": "d8a03a86b3b4a7fe52d88a302631e017",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Explaining the Confusion Matrix for Beginners\"\nsubtitle: \"Exploring, analyzing, and visualizing the confusion matrix to measure model performance with practical examples\"\nauthor: \"Yichi Zhang\"\ndate: \"2025-01-18\"\ncategories: [Explanations]\n---\n\n\n# Introduction\n\nIn the world of machine learning, classification is a wildly used supervised learning method, which recognize patterns and make predictions about which category a new piece of data belongs to. Classifiction models are used in a variety of scenarios, such as predicting if an email is spam, determining if a customer is likely to churn, or diagnosing a disease based on medical images. However, just because a model provides an answer doesn’t mean it’s the right one. The model can make mistakes when making predictions, and some mistakes are more harmful than others. For example, if the model is used to detect tumors from scans. If the model misclassified a malignant tumor as a benign one, the patient might not reciving proper treatment in time and cause some serious consequences. To truly evaluate a model’s performance, we need more than just its accuracy score. This is where the confusion matrix comes in.\n\nIn this blog post, we’ll explain the confusion matrix, how it works, how to interpret the key metrics derived from it, and how to apply it in Python to evaluate your own classification models.\n\n# What is a Confusion Matrix\n\nA confusion matrix is a table used to describe the performance of a classification model. It compares the actual values (true labels) against the predicted values, helping us understand where the model is making correct predictions and where it is making mistakes.\n\n![](confusion_matrix.png) need captain and reference\n\nFigure 1 is a generalized confusion matrix for a binary classification model. There are only two labels in the target variable: positive and negative. The positive class is defined according to the problem, it is the label that we are interested in spotting. For example, if the model is used to detect spam emails, a spam email is considered as positive class while a ham email is considered as negative class. Here are some definitions of the terms in Figure 1.\n\nTrue Positive (TP): The model predicted the positive class as positive.\nFalse Positive (FP): The model predicted the negative class as positive (Type I error).\nTrue Negative (TN): The model predicted the negative class as negative.\nFalse Negative (FN): The model predicted the positive class as negative (Type II error).\n\nIn the email classification example, a true positive means the model correctly identifies the spam email as spam. A false positive means the model marks an important email as spam. A true negative means the model identidies the ham email as ham. A false negative means the model is missing a spam email, and left it in your inbox.\n\n# Why is the Confusion Matrix useful\n\nThe confusion matrix breaks predictions into four categories, which can help data scientists understand the model performance better by knowing which kind of error the model tends to make. We can calculate a range of performance metrics by using the number of cases in the above four categories (TP, FP, TN, FN) to address different aspects of model evaluation.\n\n## Accuracy\nAccuracy is the most comment used performance metric. It is the proportion of correct predictions (both positive and negative) made by the model out of the total number of predictions.\n\n|             | Predicted Spam | Predicted Ham |\n|-------------|----------------|---------------|\n| Actual Spam |      40        |       10      |\n| Actual Ham  |      5         |       45      |\n\n: Confusion Matrix for 100 emails {.striped .hover}\n\n$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{40 + 45}{40 + 45 + 5 + 10} = \\frac{85}{100} = 0.85$\n\nThe accuracy provides a simple and high-level view of model performance. The max value of accuracy is 1. Better model always has higher accuracy. However, accuracy could be a misleading metrix. Imagine you are training the email classification model with an imbalanced dataset that contains 99 ham emails and only 1 spam emails, even the dummy model can reach an accuracy of 99% becuase it predicting every observation as ham. Although the accuracy is high, the dummy model does not have the ability to detect spam email. But don't worry, we have other metrcis to help working with imbalanced datasets.\n\n## Precision\nPrecision is the proportion of true positive predictions over all positive predictions. It measures how many of the positive predictions made by a classification model are actually correct.\n\n|             | Predicted Spam | Predicted Ham |\n|-------------|----------------|---------------|\n| Actual Spam |      40        |       10      |\n| Actual Ham  |      5         |       45      |\n\n: Confusion Matrix for 100 emails {.striped .hover}\n\n$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{40}{40 + 5} = \\frac{40}{45} = 0.89$\n\nPrecision is important in scenarios where false positives are more costly, such as the email classification model incorrectly classifies an important email as spam. The maximum value of precision is 1. Higher precision indicating fewer false positives. However, a high precision does not guarantee good model performance. In the email example, if the model predicts only one email as spam out of 100 spam emails and happens to be correct, the precision is 1 in this case, but the model fails to detect most spam emails. \n\n## Recall\nRecall is the proportion of true positive predictions over all actual positive cases. It measures how many of the actual positive cases are successfully identified.\n\n|             | Predicted Spam | Predicted Ham |\n|-------------|----------------|---------------|\n| Actual Spam |      40        |       10      |\n| Actual Ham  |      5         |       45      |\n\n: Confusion Matrix for 100 emails {.striped .hover}\n\n$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{40}{40 + 10} = \\frac{40}{50} = 0.8$\n\nRecall is important in scenarios where false negatives are more harmful. As we mentioned in the introduction, if the model misclassified a malignant tumor as a benign one (in the tumor detection, malignant tumor is usually the positive class), it  leads to a lack of treatment and reduces survival rates. The maximum value of recall is 1. A higher recall indicates fewer missed positives, but it does not necessarily indicates good model performance. In the email example, if the model predicts every email as spam, the recall will be 1 because no spam email is missing. The model would classify all ham emails incorrectly, and there would be nothing left in the inbox.\n\n## F1-Score\nYou may notice from the previous sections that there's a trade off between precision and recall. That is correct! \n\n- if the model has high precision and low recall, it indicates the model is predicting fewer positives to ensure they are correct This may miss some actual positives (high false negatives).\n\n- if the model has high recall and low precision, it implies the model is predicting many positives, catching most true positives, but also increases false positives.\n\nWe want to have a performance metric that provides a more balanced view. F1-score measures precision and recall at the same time and provides a single value that represents the trade-off. A high F1-Score indicates the model has a good balance of precision and recall.\n\n|             | Predicted Spam | Predicted Ham |\n|-------------|----------------|---------------|\n| Actual Spam |      40        |       10      |\n| Actual Ham  |      5         |       45      |\n\n: Confusion Matrix for 100 emails {.striped .hover}\n\n$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} =  2 \\cdot \\frac{0.89 \\cdot 0.8}{0.89 + 0.8} = 0.84$\n\n\n# Python example\n\nYou might overwhelmed by the calculations of each performance metric, especially when the number of observations in the dataset is large. Fortunately, we have packages and functions that help us handle all the calculations.\n\n::: {#0a932b32 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n\n# Example predictions\ny_true = [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n\n# Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Metrics\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n```\n:::\n\n\n# Conclusion\n\nNow you have a solid understanding of the confusion matrix and some performance metrics, you can try applying these concepts to your own classification models. Don’t just focus on accuracy—take the time to dig deeper into precision, recall, and F1-score to ensure your model is performing at its best. Start by building a confusion matrix in Python using your own datasets and explore how these metrics help you understand your model’s true performance.\n\n# References\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}